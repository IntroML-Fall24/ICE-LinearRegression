---
title: "In-class Case Study:  Solubility Model using Linear Regression"
author: "Name"
format: html
editor: visual
---

## Case Study: Quantitative Structure-Activity Relationship Modeling

This exercise is inspired by an example in the book *Applied Predictive Modeling* by Max Kuhn and Kjell Johnson.

Chemicals, including drugs, can be represented by chemical formulas. For example, the figure below (source: Wikipedia) shows the structure of aspirin, which contains nine carbon, eight hydrogen, and four oxygen atoms. From this configuration, quantitative measurements can be derived, such as the molecular weight, electrical charge, or surface area. These quantities are referred to as *chemical descriptors*, and there are many different types that can be derived from a chemical equation. Some are simplistic, such as the number of carbon atoms, while others are more complicated.

![A representation of aspirin, which contains carbon atoms (black balls) and hydrogen (white) and oxygen atoms (red)](img/aspirin.png){fig-alt="A representation of aspirin" width="400"}

Some characteristics of molecules cannot be analytically determined from the chemical structure. For example, one way a compound may be of medical value is if it can inhibit production of a specific protein. This is usually called the biological activity of a compound. The relationship between the chemical struture and its activity can be complex. As such, the relationship is usually determined empirically using experiments. One way to do this is to create a biological assay for the target of interest (for example, the protein). A set of compounds can then be placed into the assay and their activity, or inhibition, is measured. This activity information generates data which can be used as the training set for predictive modeling so that compounds, which may not yet exist, can be screened for activity. This process is referred to as quantitative structure-activity relationship (QSAR) modeling.

While activity is important, other characteristics need to be assessed to determine if a compound is "drug-like". Physical qualities, such as the solubility or lipophilicity ("greasiness") are evaluated as well as other properties, such as toxicity. A compound's solubility is very important if it is to be given orally or by injection.

We will use linear regression techniques to predict solubility using chemical structures. For our analyses, we will use 1,267 compounds and a set of descriptors that fall into one of three groups:

-   Binary "fingerprints" that indicate the presence or absence of a particular chemical substructure (208 descriptors fall into this category);

-   Count descriptors, such as the number of bonds or the number of bromine atoms (16 descriptors);

-   Continuous descriptors, such as molecular weight or surface area (4 descriptors)

### The Data

In some cases, we should expect correlations between descriptors. In the solubility data, for example, the surface area of a compound is calculated for regions associated with certain atoms (e.g., nitrogen or oxygen). One descriptor in the dataset measures the surface area associated with two specific elements while another uses the same elements plus two more. Given their definitions, we would expect that the two surface area predictors would be correlated.

Another relevant quality of the solubility predictors is that the count-based descriptors show a significant right skewness, which may have an impact on a linear model.

The solubility (response) was measured on the log base 10 scale.

The data has already been split into training (n = 951) and test (n = 316) sets. using random sampling. The data can be found in the `AppliedPredictiveModeling` library. You will need to first install the package by running the following command in your R console: `install.packages("AppliedPredictiveModeling")`. As you work through this exercise, you may need to add libraries to the code chunk below.

```{r libraries, warning = FALSE, message = FALSE}

library(AppliedPredictiveModeling)
library(tidyverse)
#needed to calculate skewness statistic:
library(e1071)
#needed for box-cox transformation to resolve skewness:
library(caret)
#needed for examining linear model output in a tidy way
library(tidymodels)
```

To load the data, run the code below:

```{r load-data}
data(solubility)
```

The predictors for the training and test sets are contained in data frames called `solTrainX` and `solTestX`, respectively. Each column of the data corresponds to a predictor (i.e., chemical descriptor) and the rows correspond to compounds. There are 228 columns in the data. If you view the training and test sets, you will notice a number of columns that begin with "`FP`". These columns correspond to the binary 0/1 fingerprint predictors that are associated with the presence or absence of a particular chemical structure. (Note: we will leave these binary indicator variables as type numeric for now since our goal is prediction and not inference).

The solubility response values are contained in the vectors `solTrainY` and `solTestY`.

### Part 1: Data Exploration

It is useful to explore the training data to understand the characteristics of the data prior to modeling.

1.  Choose two of the binary predictor variables and create an appropriate visualization that shows the relationship between the predictor variable and the solubility training response. You should treat the binary predictor variable as a factor in your plots. Be sure to include appropriate axis labels in your visualization. Summarize what can be learned from your visualizations.

2.  Now we will evaluate the continuous predictors for skewness (these are all of the other numeric predictors whose column names do not begin with "`FP`"). One rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 have significant skewness. Also, the **skewness statistic** can be computed. If the predictor distribution is roughly symmetric, the skewness value will be close to zero. As the distribution becomes more right skewed, the skewness statistic becomes larger. Similarly, as the distribution becomes more left skewed, the value becomes negative.

    First, we will calculate relevant summary statistics for the predictors in the training data.

```{r summary-stats, eval = FALSE}
library(skimr)
sumstatsTrain <- solTrainX %>%
  select(-(starts_with("FP"))) %>%
  skim() %>%
  tibble::as_tibble() %>%
  select(-c(numeric.hist, complete_rate, skim_type)) %>%
  rename(minimum = numeric.p0,
         maximum = numeric.p100)

sumstatsTrain
```

Below are two alternative ways to calculate a table of summary statistics for multiple variables.

```{r summary-stats-alt, eval = FALSE}
# The `tidy()` function from the broom package summarizes key objects from
# statistical output into a tibble (in this case, it puts the output from
# `summary()` into a tibble). The map_dfr() function in the `purrr` tidyverse
# package applies a function to each element, in this case a column, and
# returns a tibble.

# This method works as well to generate a table of summary statistics for
# multiple columns, but you will get a warning message to use skim instead
# of summary.  Also, map_dfr may be deprecated in the purrr package.

# first, select the non-binary predictor columns
sumstatsTrain-alt <- solTrainX %>%
  select(-(starts_with("FP"))) %>%
    map_dfr(.f = ~ broom::tidy(summary(.x)), .id = "variable")

sumstatsTrain-alt
```

```{r long-summary-stats, eval = FALSE}
# The code below gives the same result!
long_summary <- solTrainX |> 
  select(-(starts_with("FP"))) |>
  pivot_longer(MolWeight:SurfaceArea2) |> 
  group_by(name) |> 
  summarize(
    minimum = min(value),
    q1 = quantile(value, 0.25),
    median = median(value),
    mean = mean(value),
    q3 = quantile(value, 0.75),
    maximum = max(value)
  )

long_summary
```

In the code below, we will add a new column to the summary statistics table that calculates the ratio of the maximum to the minimum. What can you conclude from this calculation about the skewness of the predictors?

```{r skewRatio, warning = FALSE, eval = FALSE}

sumstatsTrain <- sumstatsTrain %>%
  mutate(skewRatio = maximum/minimum)

```

```{r skewRatio2, warning = FALSE, eval = FALSE}
# some resulting values could be infinity if the  minimum is 0.
# One solution might be to add 1e-5, or some other small number to each 0 value:

sumstatsTrain <- sumstatsTrain %>%
  mutate(minimum = if_else(minimum == 0, minimum + 0.0001, minimum),
         skewRatio = maximum/minimum)

```

\
Finally, we will calculate the skewness statistic for each predictor variable. To calculate the skewness for one predictor, you use the `skewness` function from library `e1071`:

```{r skewstat, eval = FALSE}
skewness(solTrainX$MolWeight)
```

\
The `apply` function can be used to compute the skewness statistic for multiple predictors as follows: `apply(data, 2, skewness)`, where the second argument `2` means apply to the columns. A value of `1` would apply the `skewness` function to the rows.\
\
Calculate the skewness statistic for each non-binary predictor in the training set. Hint: you first might need to select the columns you want as part of a pipe, as we did earlier. What can you conclude from the skewness statistics?

3.  You should have discovered in the previous question that most of the non-binary predictors appear to be right skewed. We will apply a [Box-Cox transformation](https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/) to the predictor variables. Be sure to read the short post linked above to understand how the Box-Cox transformation works before proceeding.\
    \
    We first apply the Box-Cox transformation to our dataset of predictors using the `preProcess()` function from the `caret` package, and then predict the value (create the transformed predictor values). Recall, however, that we only want to apply the transformation to a subset of the training set predictors. So we will first create a dataset containing only those predictors we want to transform. After applying the transformation, we will have to join the result to the binary predictor columns in the training set. After running the two code chunks below, you will have a dataset of transformed predictor variables.

```{r box-cox, eval = FALSE}
predictors <- solTrainX %>%
  select(-(starts_with("FP")))

bc_trans <- preProcess(predictors, method = c("BoxCox"))

#another type of transformation similar to Box-Cox:
#bc_trans <- preProcess(predictors, method = c("YeoJohnson"))

#apply the transformation:
trans_data <- predict(bc_trans, predictors)
```

```{r join-predictors, eval = FALSE}
#join the transformed predictor columns to the binary predictor variable columns
#to obtain a complete dataset of transformed predictor variables:

solTrainXtrans_new <- solTrainX %>%
  select(starts_with("FP")) %>%
  bind_cols(trans_data)
```

\
**Note:** The data in the `AppliedPredictiveModeling` library contains datasets of the Box-Cox transformed predictor variables. If you compare the output of the code below to the comparable columns in the dataset `solTrainXtrans`, you will see the results are very similar, however some columns in our resulting dataset were not transformed! This is because they contain zero values, and the Box-Cox transformation can only be applied to values strictly greater than 0. The Yeo-Johnson transformation is similar to the Box-Cox model but can accommodate predictors with zero and/or negative values. In the first code chunk above, the code to generate the Yeo-Johnson transformed values is given. Rather than rerunning our code at this point, we will just use the provided transformed training set `solTrainXtrans`.\
\
Also note that any transformation we apply to the training data must also be applied to the test data. To save us a little time, we will use the provided transformed test set `solTestXtrans` later to evaluate the predictive capabilities of our model.

4.  Using the transformed predictors, is it safe to assume that the relationship between the predictors and the response is linear? In the next code chunk, you will complete the code to create scatterplots of the (non-binary) predictors and the response along with a regression line from a flexible "smoother" model called **loess**. Summarize your findings from these scatterplots. Do you see any relationships that might be nonlinear? If so, these are variables we might want to consider adding interaction terms or quadratic terms for.\
    \
    See this [blogpost](https://aosmith.rbind.io/2018/08/20/automating-exploratory-plots/) for more details on the scatterplot code below.

```{r scatterplots, eval = FALSE}

# first, create a vector of the x predictor variable names
x = names(solTrainXtrans %>% select(MolWeight:SurfaceArea2))
```

```{r eval = FALSE}
#next we will write a function that can create a scatterplot based on one input value, the name of the x variable.  The same y variable, the solubility, will be used for each plot.
scatter_fun = function(x) {
     ggplot(solTrainXtrans, aes(x = .data[[x]], y = solTrainY) ) +
          geom_jitter() +
          geom_smooth(method = "loess", se = FALSE, color = "blue") +
          theme_bw() +
          labs(y = NULL)
}

```

```{r warning = FALSE, eval = FALSE}
#finally we will use a loop to generate one plot for each predictor variable in vector x
scatter_plots = map(x, \(x) scatter_fun(x))

# Below is an a way to call map that is compatible with older versions of R:
#scatter_plots = map(x, ~scatter_fun(.x))
```

```{r warning = FALSE, message = FALSE, eval = FALSE}
# last step:  display all the scatterplots at once
cowplot::plot_grid(plotlist = scatter_plots, combine = TRUE, ncol = 5, greedy = FALSE)
```

5.  Checking for collinearity: The next step in the model building process for linear regression is to identify predictors that have high pairwise correlations and to remove predictors so that no absolute pairwise correlation is greater than some pre-specified level. We will chose to remove predictors that have pairwise correlations greater than 0.9.\
    \
    Create a correlation plot or correlation matrix for the transformed non-binary predictor variables in `solTrainXtrans` and identify pairs of predictors that have a correlation greater than 0.9. Make a list of variables you are considering eliminating.

6.  Removing predictors one at a time until all pairwise correlations are below some threshold is a tedious process, especially when there are a large number of predictor variables. Later we will learn dimension reduction methods that can be applied before performing linear regression that will help with this. Also, future prediction methods that we will learn this semester are not as affected by multicollinearity, so this step may not be necessary for all methods.\
    \
    Here is some code to help remove variables with correlations above a threshold. This code will also generate a new dataset of predictor variables called `trainXfiltered`. Run the code below and examine the output. How many variables were eliminated? Were the same variables removed as you found in the previous question?

```{r eval = FALSE}
#Note: this analysis also includes the binary predictors
corThresh <- 0.9
tooHigh <- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred <- names(solTrainXtrans)[tooHigh]
trainXfiltered <- solTrainXtrans[, -tooHigh]

#Next we need to also eliminate the same variables from the test set:
testXfiltered <- solTestXtrans[, -tooHigh]
```

7.  It is finally time to fit a linear model! For our first model, we will predict solubility using all available predictors in `trainXfiltered`. Before fitting the model, we will need to add the response variable to our dataset of predictor variables. The code chunk below helps with this step. Add to the code to fit a linear model using all predictor variables. Use `summary` to find estimates of the RMSE and $R^2$. (Note that these values are likely to be highly optimistic as they have been derived by re-predicting the training set data).

```{r model1, eval = FALSE}
# add the response variable to our dataset of predictor variables:
trainData <- trainXfiltered
trainData$solubility <- solTrainY
```

8.  Now use the model generated in the previous question to predict solubility values for the new observations in `testXfiltered` using the `predict` function. Name the vector of predicted values `lmpred`.\
    \
    Compare your predicted values to the actual observed values in `solTestY`. I have given you code below that uses the `defaultSummary` function in the `caret` package to easily calculate the RMSE for the test data. Once you have calculated your vector of predicted values, you simply need to run this code chunk.

```{r eval = FALSE}
#  calculate the predicted values for the test data using the full linear model:
lmpred <- ...
```

```{r eval = FALSE}
# Collect the observed and predicted values into a data frame, then use 
# defaultSummary to estimate the test data performance:

lmValues <- data.frame(obs = solTestY, pred = lmpred)
defaultSummary(lmValues)

```

9.  Look back at the scatterplots you generated earlier and determine if an interaction term(s) or other nonlinear relationship should be added to your model (for example, if the relationship between the predictor and the response does not appear to be linear, perhaps adding a squared term or an interaction term could help). Fit your new model and compare the RMSE to the model in the previous question. Did adding additional terms help?
